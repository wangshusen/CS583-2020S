{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus2: Build a Supervised Autoencoder.\n",
    "\n",
    "### Name: [Your-Name?]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PCA and the standard autoencoder are unsupervised dimensionality reduction methods, and their learned features are not discriminative. If you build a classifier upon the low-dimenional features extracted by PCA and autoencoder, you will find the classification accuracy very poor.\n",
    "\n",
    "Linear discriminant analysis (LDA) is a traditionally supervised dimensionality reduction method for learning low-dimensional features which are highly discriminative. Likewise, can we extend autoencoder to supervised leanring?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You are required to build and train a supervised autoencoder look like the following.** You are required to add other layers properly to alleviate overfitting.\n",
    "\n",
    "\n",
    "![Network Structure](https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM5/supervised_ae.png?raw=true \"NetworkStructure\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read and run my code to train a standard dense autoencoder.\n",
    "\n",
    "2. Build and train a supervised autoencoder, visual the low-dim features and the reconstructions, and evaluate whether the learned low-dim features are discriminative.\n",
    "    \n",
    "3. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "    \n",
    "    \n",
    "    \n",
    "4. Upload this .HTML file to your Google Drive, Dropbox, or Github repo.\n",
    "\n",
    "4. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583-2020S/blob/master/homework/Bonus2/Bonus2.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 28*28).astype('float32') / 255.\n",
    "x_test = x_test.reshape(10000, 28*28).astype('float32') / 255.\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape)) \n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "print('Shape of y_train: ' + str(y_train.shape))\n",
    "print('Shape of y_test: ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def to_one_hot(y, num_class=10):\n",
    "    results = numpy.zeros((len(y), num_class))\n",
    "    for i, label in enumerate(y):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 60K training samples to 2 sets:\n",
    "* a training set containing 10K samples;\n",
    "* a validation set containing 50K samples. (You can use only 10K to save time.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_indices = numpy.random.permutation(60000)\n",
    "train_indices = rand_indices[0:10000]\n",
    "valid_indices = rand_indices[10000:20000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build an unsupervised  autoencoder and tune its hyper-parameters\n",
    "\n",
    "1. Build a dense autoencoder model\n",
    "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "    * Do NOT use test data for hyper-parameter tuning!!!\n",
    "    \n",
    "3. Try to achieve a validation loss as low as possible.\n",
    "4. Evaluate the model on the test set.\n",
    "5. Visualize the low-dim features and reconstructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input\n",
    "from keras import models\n",
    "\n",
    "input_img = Input(shape=(784,), name='input_img')\n",
    "\n",
    "encode1 = Dense(128, activation='relu', name='encode1')(input_img)\n",
    "encode2 = Dense(32, activation='relu', name='encode2')(encode1)\n",
    "encode3 = Dense(8, activation='relu', name='encode3')(encode2)\n",
    "bottleneck = Dense(2, activation='relu', name='bottleneck')(encode3)\n",
    "decode1 = Dense(8, activation='relu', name='decode1')(bottleneck)\n",
    "decode2 = Dense(32, activation='relu', name='decode2')(decode1)\n",
    "decode3 = Dense(128, activation='relu', name='decode3')(decode2)\n",
    "decode4 = Dense(784, activation='relu', name='decode4')(decode3)\n",
    "\n",
    "ae = models.Model(input_img, decode4)\n",
    "\n",
    "ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the network structure to a PDF file\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(ae, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=ae, show_shapes=False,\n",
    "    to_file='unsupervised_ae.pdf'\n",
    ")\n",
    "\n",
    "# you can find the file \"unsupervised_ae.pdf\" in the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Train the model and tune the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 1E-3 # to be tuned!\n",
    "\n",
    "ae.compile(loss='mean_squared_error',\n",
    "           optimizer=optimizers.RMSprop(lr=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ae.fit(x_tr, x_tr, \n",
    "                 batch_size=128, \n",
    "                 epochs=100, \n",
    "                 validation_data=(x_val, x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Visualize the reconstructed test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_output = ae.predict(x_test).reshape((10000, 28, 28))\n",
    "\n",
    "ROW = 5\n",
    "COLUMN = 4\n",
    "\n",
    "x = ae_output\n",
    "fname = 'reconstruct_ae.pdf'\n",
    "\n",
    "fig, axes = plt.subplots(nrows=ROW, ncols=COLUMN, figsize=(8, 10))\n",
    "for ax, i in zip(axes.flat, numpy.arange(ROW*COLUMN)):\n",
    "    image = x[i].reshape(28, 28)\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Evaluate the model on the test set\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ae.evaluate(x_test, x_test)\n",
    "print('loss = ' + str(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Visualize the low-dimensional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the encoder network\n",
    "ae_encoder = models.Model(input_img, bottleneck)\n",
    "ae_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract low-dimensional features from the test data\n",
    "encoded_test = ae_encoder.predict(x_test)\n",
    "print('Shape of encoded_test: ' + str(encoded_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = numpy.array(['r', 'g', 'b', 'm', 'c', 'k', 'y', 'purple', 'darkred', 'navy'])\n",
    "colors_test = colors[y_test]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.scatter(encoded_test[:, 0], encoded_test[:, 1], s=10, c=colors_test, edgecolors=colors_test)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "fname = 'ae_code.pdf'\n",
    "plt.savefig(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark:\n",
    "\n",
    "Judging from the visualization, the low-dim features seems not discriminative, as 2D features from different classes are mixed. Let quantatively find out whether they are discriminative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Are the learned low-dim features discriminative?\n",
    "\n",
    "To find the answer, lets train a classifier on the training set (the extracted 2-dim features) and evaluation on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the 2D features from the training, validation, and test samples\n",
    "f_tr = ae_encoder.predict(x_tr)\n",
    "f_val = ae_encoder.predict(x_val)\n",
    "f_te = ae_encoder.predict(x_test)\n",
    "\n",
    "print('Shape of f_tr: ' + str(f_tr.shape))\n",
    "print('Shape of f_te: ' + str(f_te.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input\n",
    "from keras import models\n",
    "\n",
    "input_feat = Input(shape=(2,))\n",
    "\n",
    "hidden1 = Dense(128, activation='relu')(input_feat)\n",
    "hidden2 = Dense(128, activation='relu')(hidden1)\n",
    "output = Dense(10, activation='softmax')(hidden2)\n",
    "\n",
    "classifier = models.Model(input_feat, output)\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.RMSprop(lr=1E-4),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "history = classifier.fit(f_tr, y_tr, \n",
    "                        batch_size=32, \n",
    "                        epochs=30, \n",
    "                        validation_data=(f_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Using the 2D features, the validation accuracy is 60~70%. Recall that using the original data, the accuracy is about 98%. Obviously, the 2D features are not very discriminative.\n",
    "\n",
    "We are going to build a supervised autoencode model for learning low-dimensional discriminative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a supervised autoencoder model\n",
    "\n",
    "\n",
    "**You are required to build and train a supervised autoencoder look like the following.** (Not necessary the same.) You are required to add other layers properly to alleviate overfitting.\n",
    "\n",
    "\n",
    "![Network Structure](https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM5/supervised_ae.png?raw=true \"NetworkStructure\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the supervised autoencoder network\n",
    "from keras.layers import Dense, Input\n",
    "from keras import models\n",
    "\n",
    "input_img = Input(shape=(784,), name='input_img')\n",
    "\n",
    "# encoder network\n",
    "encode1 = <add a dense layer taking input_img as input>\n",
    "<Add more layers...>\n",
    "# The width of the bottleneck layer must be exactly 2.\n",
    "bottleneck = <the output of encoder network>\n",
    "\n",
    "# decoder network\n",
    "decode1 = <add a dense layer taking bottleneck as input>\n",
    "<Add more layers...>\n",
    "decode4 = <the output of decoder network>\n",
    "\n",
    "# build a classifier upon the bottleneck layer\n",
    "classifier1 = <add a dense layer taking bottleneck as input>\n",
    "<Add more dense layers and regularizations...>\n",
    "classifier3 = <the output of classifier network>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect the input and the two outputs\n",
    "sae = models.Model(input_img, [decode4, classifier3])\n",
    "\n",
    "sae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the network structure to a PDF file\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(sae, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=sae, show_shapes=False,\n",
    "    to_file='supervised_ae.pdf'\n",
    ")\n",
    "\n",
    "# you can find the file \"supervised_ae.pdf\" in the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Train the new model and tune the hyper-parameters\n",
    "\n",
    "The new model has multiple output. Thus we specify **multiple** loss functions and their weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "sae.compile(loss=['mean_squared_error', 'categorical_crossentropy'],\n",
    "            loss_weights=[1, 0.5], # to be tuned\n",
    "            optimizer=optimizers.RMSprop(lr=1E-3))\n",
    "\n",
    "history = sae.fit(x_tr, [x_tr, y_tr], \n",
    "                  batch_size=32, \n",
    "                  epochs=100, \n",
    "                  validation_data=(x_val, [x_val, y_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Do you think overfitting is happening? If yes, what can you do? Please make necessary changes to the supervised autoencoder network structure.\n",
    "\n",
    "**Failing to add proper regularization will lose 1~2 scores.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Visualize the reconstructed test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_output = sae.predict(x_test)[0].reshape((10000, 28, 28))\n",
    "\n",
    "ROW = 5\n",
    "COLUMN = 4\n",
    "\n",
    "x = sae_output\n",
    "fname = 'reconstruct_sae.pdf'\n",
    "\n",
    "fig, axes = plt.subplots(nrows=ROW, ncols=COLUMN, figsize=(8, 10))\n",
    "for ax, i in zip(axes.flat, numpy.arange(ROW*COLUMN)):\n",
    "    image = x[i].reshape(28, 28)\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Visualize the low-dimensional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the encoder model\n",
    "sae_encoder = models.Model(input_img, bottleneck)\n",
    "sae_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract test features\n",
    "encoded_test = sae_encoder.predict(x_test)\n",
    "print('Shape of encoded_test: ' + str(encoded_test.shape))\n",
    "\n",
    "colors = numpy.array(['r', 'g', 'b', 'm', 'c', 'k', 'y', 'purple', 'darkred', 'navy'])\n",
    "colors_test = colors[y_test]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.scatter(encoded_test[:, 0], encoded_test[:, 1], s=10, c=colors_test, edgecolors=colors_test)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "fname = 'sae_code.pdf'\n",
    "plt.savefig(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Are the learned low-dim features discriminative?\n",
    "\n",
    "To find the answer, lets train a classifier on the training set (the extracted 2-dim features) and evaluation on the validation and test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract 2D features from the training, validation, and test samples\n",
    "f_tr = sae_encoder.predict(x_tr)\n",
    "f_val = sae_encoder.predict(x_val)\n",
    "f_te = sae_encoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier which takes the 2D features as input\n",
    "from keras.layers import Dense, Input\n",
    "from keras import models\n",
    "\n",
    "input_feat = Input(shape=(2,))\n",
    "\n",
    "<build a classifier which takes input_feat as input>\n",
    "output = <output of the classifier network>\n",
    "\n",
    "classifier = models.Model(input_feat, output)\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.RMSprop(lr=1E-4),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "history = classifier.fit(f_tr, y_tr, \n",
    "                        batch_size=32, \n",
    "                        epochs=30, \n",
    "                        validation_data=(f_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark:\n",
    "\n",
    "The validation accuracy must be above 90%. It means the low-dim features learned by the supervised autoencoder are very effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate your model on the never-seen-before test data\n",
    "# write your code here:\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
